{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "changed from https://www.kaggle.com/anycode/simple-nn-baseline\n",
    "\n",
    "to run this kernel, pip install ultimate first from your custom packages\n",
    "\"\"\"\n",
    "import gc, sys\n",
    "\n",
    "gc.enable()\n",
    "\n",
    "INPUT_DIR = \"../input/\"\n",
    "\n",
    "def Adam(g, epochs, alpha, beta1,beta2, w, batch_size):\n",
    "    # compute gradient module using autograd\n",
    "    gradient = grad(g)\n",
    "    d = 0\n",
    "    h  = 0\n",
    "    steps = int(np.ceil(x.shape[0] / batch_size))\n",
    "\n",
    "    # run the gradient descent loop\n",
    "    weight_history = [w]  # weight history container\n",
    "    cost_history = [g(w, range(0, x.shape[0]))]  # cost function history container\n",
    "    for j in range(epochs):\n",
    "        print('epoch:' + str(j+1) + '  cost:'+ str(cost_history[-1]))\n",
    "        for i in range(steps):\n",
    "            batch = np.arange(i * batch_size, min((i + 1) * batch_size, x.shape[0]))\n",
    "            #batch = [i * batch_size + z for z in range(0, batch_size)]\n",
    "\n",
    "            d  = beta1 * d + (1 - beta1) * gradient(w, (batch))\n",
    "\n",
    "            h = beta2 * h + (1 - beta2) * (gradient(w, (batch)) ** 2)\n",
    "            # evaluate the gradient\n",
    "\n",
    "            # take gradient descent step\n",
    "            w = w - alpha * d / (np.sqrt(h)+ 10 ** (-8))\n",
    "            w = w - (alpha * d)\n",
    "\n",
    "            # record weight and cost\n",
    "            weight_history.append(w)\n",
    "            cost_history.append(g(w, range(0, x.shape[0])))\n",
    "    return weight_history, cost_history\n",
    "\n",
    "def gradient_descent(g, epochs, alpha, bealta, w, batch_size):\n",
    "    # compute gradient module using autograd\n",
    "    gradient = grad(g)\n",
    "    d = 0\n",
    "\n",
    "    #steps = x.shape[0] / batch_size\n",
    "    #steps = int(steps)\n",
    "\n",
    "    steps = int(np.ceil(x.shape[0] / batch_size))\n",
    "\n",
    "    # run the gradient descent loop\n",
    "    weight_history = [w]  # weight history container\n",
    "    cost_history = [g(w, range(0, x.shape[0]))]  # cost function history container\n",
    "    for j in range(epochs):\n",
    "        print('epoch:' + str(j+1) + '  cost:'+ str(cost_history[-1]))\n",
    "        for i in range(steps):\n",
    "            batch = np.arange(i * batch_size, min((i + 1) * batch_size, x.shape[0]))\n",
    "            #batch = [i * batch_size + z for z in range(0, batch_size)]\n",
    "            d  = bealta * d + (1 - bealta) * gradient(w, (batch))\n",
    "            # evaluate the gradient\n",
    "\n",
    "            # take gradient descent step\n",
    "            w = w - (alpha * d)\n",
    "\n",
    "            # record weight and cost\n",
    "            weight_history.append(w)\n",
    "            cost_history.append(g(w, range(0, x.shape[0])))\n",
    "    return weight_history, cost_history\n",
    "\n",
    "def model(x_p, w):\n",
    "    a = w[0] + np.dot((x_p), w[1:])\n",
    "    return a.T\n",
    "def cost_function(w,iter):\n",
    "    x_p = x[iter,:]\n",
    "    y_p = y[iter]\n",
    "    cost = np.sum(np.sqrt((model(x_p, w) - y_p)**2))\n",
    "    #cost = np.sum(np.log(1 + np.exp(-y_p * model(x_p, w))))\n",
    "    return cost/float(np.size(y_p))\n",
    "\n",
    "\n",
    "def feature_engineering(is_train=True):\n",
    "    if is_train:\n",
    "        print(\"processing train.csv\")\n",
    "        #df = pd.read_csv(INPUT_DIR + 'train_V2.csv', nrows = 10000)\n",
    "        df = pd.read_csv(INPUT_DIR + 'train_V2.csv')\n",
    "        df = df[df['maxPlace'] > 1]\n",
    "    else:\n",
    "        print(\"processing test.csv\")\n",
    "        #df = pd.read_csv(INPUT_DIR + 'test_V2.csv', nrows = 10000)\n",
    "        df = pd.read_csv(INPUT_DIR + 'test_V2.csv')\n",
    "    # df = reduce_mem_usage(df)\n",
    "    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n",
    "\n",
    "\n",
    "\n",
    "    print(\"remove some columns\")\n",
    "    target = 'winPlacePerc'\n",
    "    features = list(df.columns)\n",
    "    features.remove(\"Id\")\n",
    "    features.remove(\"matchId\")\n",
    "    features.remove(\"groupId\")\n",
    "\n",
    "    features.remove(\"matchType\")\n",
    "\n",
    "\n",
    "    y = None\n",
    "\n",
    "    print(\"get target\")\n",
    "    if is_train:\n",
    "        y = np.array(df.groupby(['matchId', 'groupId'])[target].agg('mean'), dtype=np.float64)\n",
    "        features.remove(target)\n",
    "\n",
    "    print(\"get group mean feature\")\n",
    "    agg = df.groupby(['matchId', 'groupId'])[features].agg('mean')\n",
    "    #agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "\n",
    "    if is_train:\n",
    "        df_out = agg.reset_index()[['matchId', 'groupId']]\n",
    "    else:\n",
    "        df_out = df[['matchId', 'groupId']]\n",
    "\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "   # df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "\n",
    "\n",
    "    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n",
    "\n",
    "    X = np.array(df_out, dtype=np.float64)\n",
    "\n",
    "    feature_names = list(df_out.columns)\n",
    "\n",
    "    del df, df_out, agg\n",
    "    gc.collect()\n",
    "\n",
    "    return X, y, feature_names\n",
    "\n",
    "\n",
    "data_x, y, feature_names = feature_engineering(True)\n",
    "#y = y * 2 - 1\n",
    "w0 = 0.1* np.random.rand(data_x.shape[1]+ 1, 1)\n",
    "\n",
    "x_means = np.mean(data_x,axis = 0)[np.newaxis,:]\n",
    "x_stds = np.std(data_x,axis = 0)[np.newaxis,:]\n",
    "x = (data_x - x_means)/(x_stds+0.0000001)\n",
    "\n",
    "w_history, cost_history = Adam(cost_function, 30, 0.001, 0 ,0.999, w0, 10240)# RMSprop beta1 = 0\n",
    "w1 = w_history[-1]\n",
    "print (cost_history[-1])\n",
    "\n",
    "plt.plot(cost_history)\n",
    "plt.show()\n",
    "x_test, _, _ = feature_engineering(False)\n",
    "x_means = np.mean(x_test,axis = 0)[np.newaxis,:]\n",
    "x_stds = np.std(x_test,axis = 0)[np.newaxis,:]\n",
    "x_test = (x_test - x_means)/(x_stds+0.0000001)\n",
    "print(x_test.shape)\n",
    "\n",
    "y_predict =  model(x_test, w1)\n",
    "y_predict = y_predict.reshape(-1,1)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(y_predict)\n",
    "y_predict = scaler.transform(y_predict)\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(INPUT_DIR + 'test_V2.csv')\n",
    "\n",
    "df_test['winPlacePerc'] = y_predict\n",
    "\n",
    "for i in range(len(df_test)):\n",
    "    winPlacePerc = y_predict[i][0]\n",
    "    maxPlace = int(df_test.iloc[i]['maxPlace'])\n",
    "    if maxPlace == 0:\n",
    "        winPlacePerc = 0.0\n",
    "    elif maxPlace == 1:\n",
    "        winPlacePerc = 1.0\n",
    "    else:\n",
    "        gap = 1.0 / (maxPlace - 1)\n",
    "        winPlacePerc = round(winPlacePerc / gap) * gap\n",
    "\n",
    "    if winPlacePerc < 0: winPlacePerc = 0.0\n",
    "    if winPlacePerc > 1: winPlacePerc = 1.0\n",
    "    y_predict[i][0] = winPlacePerc\n",
    "\n",
    "a = np.sum(y_predict)/float(len(y_predict))\n",
    "#y_predict\n",
    "df_test['winPlacePerc'] = y_predict\n",
    "submission = df_test[['Id', 'winPlacePerc']]\n",
    "submission.to_csv('RsmProp-AE.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
