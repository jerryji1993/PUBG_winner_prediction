{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autograd import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import gc, sys\n",
    "import time\n",
    "gc.enable()\n",
    "\n",
    "INPUT_DIR = \"../input/\"\n",
    "\n",
    "def Adam(g, epochs, alpha, beta1,beta2, w, batch_size):\n",
    "    # compute gradient module using autograd\n",
    "    gradient = grad(g)\n",
    "    d = 0\n",
    "    h  = 0\n",
    "    steps = int(np.ceil(x.shape[0] / batch_size))\n",
    "\n",
    "    # run the gradient descent loop\n",
    "    weight_history = [w]  # weight history container\n",
    "    cost_history = [g(w, range(0, x.shape[0]))]  # cost function history container\n",
    "    for j in range(epochs):\n",
    "        print('epoch:' + str(j+1) + '  cost:'+ str(cost_history[-1]))\n",
    "        for i in range(steps):\n",
    "            batch = np.arange(i * batch_size, min((i + 1) * batch_size, x.shape[0]))\n",
    "            #batch = [i * batch_size + z for z in range(0, batch_size)]\n",
    "\n",
    "            d  = beta1 * d + (1 - beta1) * gradient(w, (batch))\n",
    "\n",
    "            h = beta2 * h + (1 - beta2) * (gradient(w, (batch)) ** 2)\n",
    "            # evaluate the gradient\n",
    "\n",
    "            # take gradient descent step\n",
    "            w = w - alpha * d / (np.sqrt(h)+ 10 ** (-8))\n",
    "            w = w - (alpha * d)\n",
    "\n",
    "            # record weight and cost\n",
    "            weight_history.append(w)\n",
    "            cost_history.append(g(w, range(0, x.shape[0])))\n",
    "    return weight_history, cost_history\n",
    "\n",
    "def gradient_descent(g, epochs, alpha, bealta, w, batch_size):\n",
    "    # compute gradient module using autograd\n",
    "    gradient = grad(g)\n",
    "    d = 0\n",
    "\n",
    "    #steps = x.shape[0] / batch_size\n",
    "    #steps = int(steps)\n",
    "\n",
    "    steps = int(np.ceil(x.shape[0] / batch_size))\n",
    "\n",
    "    # run the gradient descent loop\n",
    "    weight_history = [w]  # weight history container\n",
    "    cost_history = [g(w, range(0, x.shape[0]))]  # cost function history container\n",
    "    for j in range(epochs):\n",
    "        print('epoch:' + str(j+1) + '  cost:'+ str(cost_history[-1]))\n",
    "        for i in range(steps):\n",
    "            \n",
    "            batch = np.arange(i * batch_size, min((i + 1) * batch_size, x.shape[0]))\n",
    "            \n",
    "            d  = bealta * d + (1 - bealta) * gradient(w, (batch))\n",
    "            \n",
    "            \n",
    "            # take gradient descent step\n",
    "            w = w - (alpha * d)\n",
    "            \n",
    "            # record weight and cost\n",
    "        weight_history.append(w)\n",
    "        cost_history.append(g(w, range(0, x.shape[0])))\n",
    "    return weight_history, cost_history\n",
    "\n",
    "def model(x_p, w):\n",
    "    a = w[0] + np.dot((x_p), w[1:])\n",
    "    return a.T\n",
    "\t\n",
    "def cost_function(w,iter):\n",
    "    x_p = x[iter,:]\n",
    "    y_p = y[iter]\n",
    "    cost = np.sum(np.maximum(0, -y_p * model(x_p, w)))/float(np.size(y_p))\n",
    "    return cost\n",
    "\n",
    "\n",
    "def feature_engineering(is_train=True):\n",
    "    if is_train:\n",
    "        print(\"processing train.csv\")\n",
    "        # df = pd.read_csv(INPUT_DIR + 'train_V2.csv', nrows = 10000)\n",
    "        df = pd.read_csv(INPUT_DIR + 'train_V2.csv')\n",
    "        df = df[df['maxPlace'] > 1]\n",
    "    else:\n",
    "        print(\"processing test.csv\")\n",
    "        #df = pd.read_csv(INPUT_DIR + 'test_V2.csv', nrows = 10000)\n",
    "        df = pd.read_csv(INPUT_DIR + 'test_V2.csv')\n",
    "    # df = reduce_mem_usage(df)\n",
    "    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n",
    "\n",
    "\n",
    "\n",
    "    print(\"remove some columns\")\n",
    "    target = 'winPlacePerc'\n",
    "    features = list(df.columns)\n",
    "    features.remove(\"Id\")\n",
    "    features.remove(\"matchId\")\n",
    "    features.remove(\"groupId\")\n",
    "\n",
    "    features.remove(\"matchType\")\n",
    "\n",
    "\n",
    "    y = None\n",
    "\n",
    "    print(\"get target\")\n",
    "    if is_train:\n",
    "        y = np.array(df.groupby(['matchId', 'groupId'])[target].agg('mean'), dtype=np.float64)\n",
    "        features.remove(target)\n",
    "\n",
    "    print(\"get group mean feature\")\n",
    "    agg = df.groupby(['matchId', 'groupId'])[features].agg('mean')\n",
    "    #agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "\n",
    "    if is_train:\n",
    "        df_out = agg.reset_index()[['matchId', 'groupId']]\n",
    "    else:\n",
    "        df_out = df[['matchId', 'groupId']]\n",
    "\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "   # df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "\n",
    "\n",
    "    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n",
    "\n",
    "    X = np.array(df_out, dtype=np.float64)\n",
    "\n",
    "    feature_names = list(df_out.columns)\n",
    "\n",
    "    del df, df_out, agg\n",
    "    gc.collect()\n",
    "\n",
    "    return X, y, feature_names\n",
    "\n",
    "\n",
    "data_x, y, feature_names = feature_engineering(True)\n",
    "w0 = np.random.rand(data_x.shape[1]+ 1, 1)\n",
    "\n",
    "x_means = np.mean(data_x,axis = 0)[np.newaxis,:]\n",
    "x_stds = np.std(data_x,axis = 0)[np.newaxis,:]\n",
    "x = (data_x - x_means)/(x_stds+0.0000001)\n",
    "print (x.shape)\n",
    "# y = 2 * y - 1\n",
    "for i in range(len(y)):\n",
    "\tif y[i] < 0.7:\n",
    "\t\ty[i] = -1\n",
    "\telse:\n",
    "\t\ty[i] = 1\n",
    "\n",
    "w_history, cost_history = Adam(cost_function, 30, 0.1, 0 ,0.999, w0, 10240)#Rsmprop belt1 = 0\n",
    "#w_history, cost_history = gradient_descent(cost_function, 100, 0.1, 0.9 , w0, 64)\n",
    "w1 = w_history[-1]\n",
    "print (cost_history[-1])\n",
    "\n",
    "plt.plot(cost_history)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "x_test, _, _ = feature_engineering(False)\n",
    "x_means = np.mean(x_test,axis = 0)[np.newaxis,:]\n",
    "x_stds = np.std(x_test,axis = 0)[np.newaxis,:]\n",
    "x_test = (x_test - x_means)/(x_stds+0.0000001)\n",
    "print(x_test.shape)\n",
    "\n",
    "y_predict =  model(x_test, w1)\n",
    "y_predict = y_predict.reshape(-1,1)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "print(y_predict.shape)\n",
    "scaler.fit(y_predict)\n",
    "y_predict = scaler.transform(y_predict)\n",
    "\n",
    "df_test = pd.read_csv(INPUT_DIR + 'test_V2.csv')\n",
    "\n",
    "df_test['winPlacePerc'] = y_predict\n",
    "\n",
    "for i in range(len(df_test)):\n",
    "    winPlacePerc = y_predict[i][0]\n",
    "    maxPlace = int(df_test.iloc[i]['maxPlace'])\n",
    "    if maxPlace == 0:\n",
    "        winPlacePerc = 0.0\n",
    "    elif maxPlace == 1:\n",
    "        winPlacePerc = 1.0\n",
    "    else:\n",
    "        gap = 1.0 / (maxPlace - 1)\n",
    "        winPlacePerc = round(winPlacePerc / gap) * gap\n",
    "\n",
    "    if winPlacePerc < 0: winPlacePerc = 0.0\n",
    "    if winPlacePerc > 1: winPlacePerc = 1.0\n",
    "    y_predict[i][0] = winPlacePerc\n",
    "\n",
    "# a = np.sum(y_predict)/float(len(y_predict))\n",
    "\n",
    "df_test['winPlacePerc'] = y_predict\n",
    "submission = df_test[['Id', 'winPlacePerc']]\n",
    "submission.to_csv('PerceptronRsmprop.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
