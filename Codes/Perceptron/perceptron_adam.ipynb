{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from autograd import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom autograd import grad\nimport matplotlib.pyplot as plt\n\n\nimport gc, sys\nimport time\ngc.enable()\n\nINPUT_DIR = \"../input/\"\n\ndef Adam(g, epochs, alpha, beta1,beta2, w, batch_size):\n    # compute gradient module using autograd\n    gradient = grad(g)\n    d = 0\n    h  = 0\n    steps = int(np.ceil(x.shape[0] / batch_size))\n\n    # run the gradient descent loop\n    weight_history = [w]  # weight history container\n    cost_history = [g(w, range(0, x.shape[0]))]  # cost function history container\n    for j in range(epochs):\n        print('epoch:' + str(j+1) + '  cost:'+ str(cost_history[-1]))\n        for i in range(steps):\n            batch = np.arange(i * batch_size, min((i + 1) * batch_size, x.shape[0]))\n            #batch = [i * batch_size + z for z in range(0, batch_size)]\n\n            d  = beta1 * d + (1 - beta1) * gradient(w, (batch))\n\n            h = beta2 * h + (1 - beta2) * (gradient(w, (batch)) ** 2)\n            # evaluate the gradient\n\n            # take gradient descent step\n            w = w - alpha * d / (np.sqrt(h)+ 10 ** (-8))\n            w = w - (alpha * d)\n\n            # record weight and cost\n            weight_history.append(w)\n            cost_history.append(g(w, range(0, x.shape[0])))\n    return weight_history, cost_history\n\ndef gradient_descent(g, epochs, alpha, bealta, w, batch_size):\n    # compute gradient module using autograd\n    gradient = grad(g)\n    d = 0\n\n    #steps = x.shape[0] / batch_size\n    #steps = int(steps)\n\n    steps = int(np.ceil(x.shape[0] / batch_size))\n\n    # run the gradient descent loop\n    weight_history = [w]  # weight history container\n    cost_history = [g(w, range(0, x.shape[0]))]  # cost function history container\n    for j in range(epochs):\n        print('epoch:' + str(j+1) + '  cost:'+ str(cost_history[-1]))\n        for i in range(steps):\n            \n            batch = np.arange(i * batch_size, min((i + 1) * batch_size, x.shape[0]))\n            \n            d  = bealta * d + (1 - bealta) * gradient(w, (batch))\n            \n            \n            # take gradient descent step\n            w = w - (alpha * d)\n            \n            # record weight and cost\n        weight_history.append(w)\n        cost_history.append(g(w, range(0, x.shape[0])))\n    return weight_history, cost_history\n\ndef model(x_p, w):\n    a = w[0] + np.dot((x_p), w[1:])\n    return a.T\n\t\ndef cost_function(w,iter):\n    x_p = x[iter,:]\n    y_p = y[iter]\n    cost = np.sum(np.maximum(0, -y_p * model(x_p, w)))/float(np.size(y_p))\n    return cost\n\n\ndef feature_engineering(is_train=True):\n    if is_train:\n        print(\"processing train.csv\")\n        # df = pd.read_csv(INPUT_DIR + 'train_V2.csv', nrows = 10000)\n        df = pd.read_csv(INPUT_DIR + 'train_V2.csv')\n        df = df[df['maxPlace'] > 1]\n    else:\n        print(\"processing test.csv\")\n        #df = pd.read_csv(INPUT_DIR + 'test_V2.csv', nrows = 10000)\n        df = pd.read_csv(INPUT_DIR + 'test_V2.csv')\n    # df = reduce_mem_usage(df)\n    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n\n\n\n    print(\"remove some columns\")\n    target = 'winPlacePerc'\n    features = list(df.columns)\n    features.remove(\"Id\")\n    features.remove(\"matchId\")\n    features.remove(\"groupId\")\n\n    features.remove(\"matchType\")\n\n\n    y = None\n\n    print(\"get target\")\n    if is_train:\n        y = np.array(df.groupby(['matchId', 'groupId'])[target].agg('mean'), dtype=np.float64)\n        features.remove(target)\n\n    print(\"get group mean feature\")\n    agg = df.groupby(['matchId', 'groupId'])[features].agg('mean')\n    #agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n\n    if is_train:\n        df_out = agg.reset_index()[['matchId', 'groupId']]\n    else:\n        df_out = df[['matchId', 'groupId']]\n\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n   # df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n\n\n    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n\n    X = np.array(df_out, dtype=np.float64)\n\n    feature_names = list(df_out.columns)\n\n    del df, df_out, agg\n    gc.collect()\n\n    return X, y, feature_names\n\n\ndata_x, y, feature_names = feature_engineering(True)\nw0 = 0.1* np.random.rand(data_x.shape[1] + 1, 1)\n\nx_means = np.mean(data_x,axis = 0)[np.newaxis,:]\nx_stds = np.std(data_x,axis = 0)[np.newaxis,:]\nx = (data_x - x_means)/(x_stds+0.0000001)\nprint (x.shape)\n# y = 2 * y - 1\nfor i in range(len(y)):\n    if y[i] < 0.5:\n        y[i] = -1\n    else:\n        y[i] = 1\n\nw_history, cost_history = Adam(cost_function, 30, 0.01, 0.85 ,0.999, w0, 32768)\n#w_history, cost_history = gradient_descent(cost_function, 100, 0.1, 0.9 , w0, 64)\nw1 = w_history[-1]\nprint (cost_history[-1])\n\nplt.plot(cost_history)\nplt.show()\n\n\nx_test, _, _ = feature_engineering(False)\nx_means = np.mean(x_test,axis = 0)[np.newaxis,:]\nx_stds = np.std(x_test,axis = 0)[np.newaxis,:]\nx_test = (x_test - x_means)/(x_stds+0.0000001)\nprint(x_test.shape)\n\ny_predict =  model(x_test, w1)\ny_predict = y_predict.reshape(-1,1)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nprint(y_predict.shape)\nscaler.fit(y_predict)\ny_predict = scaler.transform(y_predict)\n\ndf_test = pd.read_csv(INPUT_DIR + 'test_V2.csv')\n\ndf_test['winPlacePerc'] = y_predict\n\nfor i in range(len(df_test)):\n    winPlacePerc = y_predict[i][0]\n    maxPlace = int(df_test.iloc[i]['maxPlace'])\n    if maxPlace == 0:\n        winPlacePerc = 0.0\n    elif maxPlace == 1:\n        winPlacePerc = 1.0\n    else:\n        gap = 1.0 / (maxPlace - 1)\n        winPlacePerc = round(winPlacePerc / gap) * gap\n\n    if winPlacePerc < 0: winPlacePerc = 0.0\n    if winPlacePerc > 1: winPlacePerc = 1.0\n    y_predict[i][0] = winPlacePerc\n\n# a = np.sum(y_predict)/float(len(y_predict))\n\ndf_test['winPlacePerc'] = y_predict\nsubmission = df_test[['Id', 'winPlacePerc']]\nsubmission.to_csv('PerceptronAdam.csv', index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}