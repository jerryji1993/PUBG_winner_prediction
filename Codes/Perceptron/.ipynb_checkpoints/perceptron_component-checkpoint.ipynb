{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autograd import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import gc, sys\n",
    "import time\n",
    "gc.enable()\n",
    "\n",
    "INPUT_DIR = \"../input/\"\n",
    "\n",
    "def gradient_descent(g, epochs, alpha, bealta, w, batch_size):\n",
    "    # compute gradient module using autograd\n",
    "    gradient = grad(g)\n",
    "    # d = 0\n",
    "\n",
    "    #steps = x.shape[0] / batch_size\n",
    "    #steps = int(steps)\n",
    "\n",
    "    steps = int(np.ceil(x.shape[0] / batch_size))\n",
    "\n",
    "    # run the gradient descent loop\n",
    "    weight_history = [w]  # weight history container\n",
    "    cost_history = [g(w, range(0, x.shape[0]))]  # cost function history container\n",
    "    for j in range(epochs):\n",
    "        print('epoch:' + str(j+1) + '  cost:'+ str(cost_history[-1]))\n",
    "        for i in range(steps):\n",
    "            batch = np.arange(i * batch_size, min((i + 1) * batch_size, x.shape[0]))\n",
    "            \n",
    "            # take gradient descent step\n",
    "            grad_eval = gradient(w, (batch))\n",
    "            # normalized_term = np.linalg.norm(grad_eval)\n",
    "\n",
    "            grad_eval = grad_eval / (np.abs(grad_eval) + 10**(-8))\n",
    "\n",
    "            w = w - alpha * grad_eval\n",
    "\n",
    "            # w = w - (alpha/(normalized_term+10**(-16))) * grad_eval \n",
    "\n",
    "            # record weight and cost\n",
    "        weight_history.append(w)\n",
    "        cost_history.append(g(w, range(0, x.shape[0])))\n",
    "    return weight_history, cost_history\n",
    "\n",
    "def model(x_p, w):\n",
    "    a = w[0] + np.dot((x_p), w[1:])\n",
    "    return a.T\n",
    "\t\n",
    "def cost_function(w,iter):\n",
    "    x_p = x[iter,:]\n",
    "    y_p = y[iter]\n",
    "    cost = np.sum(np.maximum(-y_p*model(x_p,w),0))/float(np.size(y_p))              # Perceptron cost\n",
    "    # cost = np.sum(np.log(1 + np.exp(-y_p * model(x_p, w))))/float(np.size(y_p))\n",
    "    return cost\n",
    "\n",
    "\n",
    "def feature_engineering(is_train=True):\n",
    "    if is_train:\n",
    "        print(\"processing train.csv\")\n",
    "        # df = pd.read_csv(INPUT_DIR + 'train_V2.csv', nrows = 10000)\n",
    "        df = pd.read_csv(INPUT_DIR + 'train_V2.csv')\n",
    "        df = df[df['maxPlace'] > 1]\n",
    "    else:\n",
    "        print(\"processing test.csv\")\n",
    "        #df = pd.read_csv(INPUT_DIR + 'test_V2.csv', nrows = 10000)\n",
    "        df = pd.read_csv(INPUT_DIR + 'test_V2.csv')\n",
    "    # df = reduce_mem_usage(df)\n",
    "    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n",
    "\n",
    "\n",
    "\n",
    "    print(\"remove some columns\")\n",
    "    target = 'winPlacePerc'\n",
    "    features = list(df.columns)\n",
    "    features.remove(\"Id\")\n",
    "    features.remove(\"matchId\")\n",
    "    features.remove(\"groupId\")\n",
    "\n",
    "    features.remove(\"matchType\")\n",
    "\n",
    "\n",
    "    y = None\n",
    "\n",
    "    print(\"get target\")\n",
    "    if is_train:\n",
    "        y = np.array(df.groupby(['matchId', 'groupId'])[target].agg('mean'), dtype=np.float64)\n",
    "        features.remove(target)\n",
    "\n",
    "    print(\"get group mean feature\")\n",
    "    agg = df.groupby(['matchId', 'groupId'])[features].agg('mean')\n",
    "    #agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "\n",
    "    if is_train:\n",
    "        df_out = agg.reset_index()[['matchId', 'groupId']]\n",
    "    else:\n",
    "        df_out = df[['matchId', 'groupId']]\n",
    "\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "   # df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "\n",
    "\n",
    "    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n",
    "\n",
    "    X = np.array(df_out, dtype=np.float64)\n",
    "\n",
    "    feature_names = list(df_out.columns)\n",
    "\n",
    "    del df, df_out, agg\n",
    "    gc.collect()\n",
    "\n",
    "    return X, y, feature_names\n",
    "\n",
    "\n",
    "data_x, y, feature_names = feature_engineering(True)\n",
    "w0 = 0.1* np.random.rand(data_x.shape[1]+ 1, 1)\n",
    "\n",
    "x_means = np.mean(data_x,axis = 0)[np.newaxis,:]\n",
    "x_stds = np.std(data_x,axis = 0)[np.newaxis,:]\n",
    "x = (data_x - x_means)/(x_stds+0.0000001)\n",
    "print(x.shape)\n",
    "#y = 2 * y - 1\n",
    "for i in range(len(y)):\n",
    "    if y[i] < 0.5:\n",
    "        y[i] = -1\n",
    "    else:\n",
    "        y[i] = 1\n",
    "\n",
    "w_history, cost_history = gradient_descent(cost_function, 200, 0.01, 0.8 , w0, 10240)\n",
    "\n",
    "# w_history, cost_history = gradient_descent(cost_function, 20, 0.1, 0.8, w0, 64)\n",
    "w1 = w_history[-1]\n",
    "print (cost_history[-1])\n",
    "\n",
    "plt.plot(cost_history)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "x_test, _, _ = feature_engineering(False)\n",
    "x_means = np.mean(x_test,axis = 0)[np.newaxis,:]\n",
    "x_stds = np.std(x_test,axis = 0)[np.newaxis,:]\n",
    "x_test = (x_test - x_means)/(x_stds+0.0000001)\n",
    "print(x_test.shape)\n",
    "\n",
    "y_predict =  model(x_test, w1)\n",
    "y_predict = y_predict.reshape(-1, 1)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "print(y_predict.shape)\n",
    "scaler.fit(y_predict)\n",
    "y_predict = scaler.transform(y_predict)\n",
    "\n",
    "df_test = pd.read_csv(INPUT_DIR + 'test_V2.csv')\n",
    "\n",
    "df_test['winPlacePerc'] = y_predict\n",
    "\n",
    "for i in range(len(df_test)):\n",
    "    winPlacePerc = y_predict[i][0]\n",
    "    maxPlace = int(df_test.iloc[i]['maxPlace'])\n",
    "    if maxPlace == 0:\n",
    "        winPlacePerc = 0.0\n",
    "    elif maxPlace == 1:\n",
    "        winPlacePerc = 1.0\n",
    "    else:\n",
    "        gap = 1.0 / (maxPlace - 1)\n",
    "        winPlacePerc = round(winPlacePerc / gap) * gap\n",
    "\n",
    "    if winPlacePerc < 0: winPlacePerc = 0.0\n",
    "    if winPlacePerc > 1: winPlacePerc = 1.0\n",
    "    y_predict[i][0] = winPlacePerc\n",
    "\n",
    "# a = np.sum(y_predict)/float(len(y_predict))\n",
    "\n",
    "df_test['winPlacePerc'] = y_predict\n",
    "submission = df_test[['Id', 'winPlacePerc']]\n",
    "submission.to_csv('PerceptronNormalized.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
