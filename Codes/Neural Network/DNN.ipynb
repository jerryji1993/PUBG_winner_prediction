{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nInput = \"../input/\"\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "ori_data = pd.read_csv(Input + \"train_V2.csv\")\nori_test = pd.read_csv(Input + \"test_V2.csv\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4979388e439d34a0919599cec0b50c9b96e3c455"
      },
      "cell_type": "code",
      "source": "print (ori_data.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a2226f99ca6149c478570e4e32866aec3215a7d4"
      },
      "cell_type": "code",
      "source": "print (ori_test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "761a2f3626bd049789350065b2d6f0d61df02b3d"
      },
      "cell_type": "code",
      "source": "# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n                    \n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c2007c269d78ac981aa36ec51e406164e5d21ca7"
      },
      "cell_type": "code",
      "source": "ori_data = reduce_mem_usage(ori_data)\nori_test = reduce_mem_usage(ori_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b20e7e9c870107ad5f4b6c5bded9f5f03dd82efb"
      },
      "cell_type": "code",
      "source": "# changed from https://www.kaggle.com/anycode/simple-nn-baseline\n# Adding features based on https://www.kaggle.com/powercode/sklearn-mlp\ndef feature_engineering(is_train=True, df_train = ori_data, df_test = ori_test):\n    if is_train: \n        print(\"processing train.csv\")\n        df = df_train\n        df = df[df['maxPlace'] > 1]\n    else:\n        print(\"processing test.csv\")\n        df = df_test\n    \n    print (\"Adding some features\")\n    \n    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n    df[\"skill\"] = df[\"headshotKills\"] + df[\"roadKills\"]\n    df['healthitems'] = df['heals'] + df['boosts']\n    \n    '''\n    df['killPlace_over_maxPlace'] = df['killPlace'] / df['maxPlace']\n    df['headshotKills_over_kills'] = df['headshotKills'] / df['kills']\n    df['distance_over_weapons'] = df['totalDistance'] / df['weaponsAcquired']\n    df['walkDistance_over_heals'] = df['walkDistance'] / df['heals']\n    df['walkDistance_over_kills'] = df['walkDistance'] / df['kills']\n    df['killsPerWalkDistance'] = df['kills'] / df['walkDistance']\n    df['headshotrate'] = df['kills']/df['headshotKills']\n    df['killStreakrate'] = df['killStreaks']/df['kills']\n    \n    df[df == np.Inf] = np.NaN\n    df[df == np.NINF] = np.NaN\n    \n    print(\"Removing Na's from df\")\n    df.fillna(0, inplace=True)\n    '''\n    # df = df[:100]\n    \n    print(\"remove some columns\")\n    target = 'winPlacePerc'\n    features = list(df.columns)\n    features.remove(\"Id\")\n    features.remove(\"matchId\")\n    features.remove(\"groupId\")\n    features.remove(\"matchType\")\n    \n    # matchType = pd.get_dummies(df['matchType'])\n    # df = df.join(matchType)    \n    \n    y = None\n    \n    print(\"get target\")\n    if is_train: \n        y = np.array(df.groupby(['matchId','groupId'])[target].agg('mean'), dtype=np.float64)\n        features.remove(target)\n\n    print(\"get group mean feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    if is_train: df_out = agg.reset_index()[['matchId','groupId']]\n    else: df_out = df[['matchId','groupId']]\n\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    print(\"get group max feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    print(\"get group min feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    print(\"get group size feature\")\n    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n    \n    print(\"get match mean feature\")\n    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n    \n    print(\"get match size feature\")\n    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n    df_out = df_out.merge(agg, how='left', on=['matchId'])\n    \n    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n\n    X = np.array(df_out, dtype=np.float64)\n    \n    feature_names = list(df_out.columns)\n\n    del df, df_out, agg, agg_rank\n    gc.collect()\n\n    return X, y, feature_names",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8b0e17888ebedefbce035cbd757ad66ad2a1d1ba"
      },
      "cell_type": "code",
      "source": "X_train, y_train, feature_names = feature_engineering()\nprint (len(feature_names))\nprint (X_train.shape)\nprint (y_train.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "18c812e6a69eabdfeb7785463bb306e2679256d6"
      },
      "cell_type": "code",
      "source": "print (feature_names)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5570836b2d696220138af638ec153d7c1765f23c"
      },
      "cell_type": "code",
      "source": "from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization, Input, Activation\nfrom keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import optimizers\nfrom keras.models import load_model, Model\nimport keras\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "86814c22e98b66ff2763b893e6ec8f3d2210f7a5"
      },
      "cell_type": "code",
      "source": "'''\nmodel = Sequential()\nmodel.add(Dense(512, kernel_initializer='he_normal', input_dim=X_train.shape[1], activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(256, kernel_initializer='he_normal', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(128, kernel_initializer='he_normal', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n'''\n\nmain_input = Input(shape=(X_train.shape[1],), name='main_input')\nx = Dense(512, kernel_initializer='he_normal')(main_input)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.LeakyReLU(alpha=0.3)(x)\n# temp_1 = x\n# got 1\nx = Dense(512, kernel_initializer='he_normal')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.LeakyReLU(alpha=0.3)(x)\n# got 2\nx = Dense(512, kernel_initializer='he_normal')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.LeakyReLU(alpha=0.3)(x)\n# got 3\nx = Dense(512, kernel_initializer='he_normal')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.LeakyReLU(alpha=0.3)(x)\n# got 4\nx = Dense(256, kernel_initializer='he_normal')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.LeakyReLU(alpha=0.3)(x)\n# temp_2 = x\n# got 5\nx = Dense(256, kernel_initializer='he_normal')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.LeakyReLU(alpha=0.3)(x)\n# got 6\nx = Dense(256, kernel_initializer='he_normal')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.LeakyReLU(alpha=0.3)(x)\n# got 7\nx = Dense(256, kernel_initializer='he_normal')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.LeakyReLU(alpha=0.3)(x)\n# got 8\nx = Dense(128, kernel_initializer='he_normal')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.LeakyReLU(alpha=0.3)(x)\n#temp_3 = x\n# got 9\nx = Dense(128, kernel_initializer='he_normal')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.LeakyReLU(alpha=0.3)(x)\n# got 10\nx = Dense(128, kernel_initializer='he_normal')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.LeakyReLU(alpha=0.3)(x)\n# got 11\nx = Dense(128, kernel_initializer='he_normal')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.LeakyReLU(alpha=0.3)(x)\n# got 12\nx = Dense(64, kernel_initializer='he_normal')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.LeakyReLU(alpha=0.3)(x)\n# got 13\nx = Dense(64, kernel_initializer='he_normal')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.LeakyReLU(alpha=0.3)(x)\n# got 14\nx = Dense(64, kernel_initializer='he_normal')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.LeakyReLU(alpha=0.3)(x)\n# got 15\nx = Dense(64, kernel_initializer='he_normal')(x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.LeakyReLU(alpha=0.3)(x)\n# got 16\nout = Dense(1, kernel_initializer='normal', activation='sigmoid')(x)\n# got output\nmodel = Model(inputs=main_input, outputs=out)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b0f00bae9917ed0748b0abc77678dd1fab6080e8"
      },
      "cell_type": "code",
      "source": "print (X_train.shape)\nprint (y_train.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1cb05d30e7a5504512ba1c61d0944202a9e9410c"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\nx_train, x_valid, label_train, label_valid = train_test_split(X_train, y_train, test_size = 0.2, random_state = 1234)\ndel X_train, y_train\ngc.collect()\nprint (\"Finishing Split!\")\n# scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(x_train)\nscaler = preprocessing.QuantileTransformer().fit(x_train)\n\nx_train = scaler.transform(x_train)\nx_valid = scaler.transform(x_valid)\nX_test, _, _ = feature_engineering(is_train = False)\nX_test = scaler.transform(X_test)\n\nprint(\"x_train\", x_train.shape, x_train.min(), x_train.max())\nprint(\"x_valid\", x_valid.shape, x_valid.min(), x_valid.max())\nprint(\"X_test\", X_test.shape, X_test.min(), X_test.max())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "618ef68b8f93f4445a7e09086a1bcc79dc5b0d62"
      },
      "cell_type": "code",
      "source": "optimizer = optimizers.Adam(lr=0.01, epsilon=1e-8, decay=1e-4, amsgrad=True)\n# optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n\nmodel.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n\ndef step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10, verbose=0):\n    '''\n    Wrapper function to create a LearningRateScheduler with step decay schedule.\n    '''\n    def schedule(epoch):\n        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n    \n    return LearningRateScheduler(schedule, verbose)\n\nlr_sched = step_decay_schedule(initial_lr=0.1, decay_factor=0.9, step_size=1, verbose=1)\n# early_stopping = EarlyStopping(monitor='val_mean_absolute_error', min_delta=0, mode = 'min', patience=8, verbose=1)\n\nhistory = model.fit(x_train, label_train, \n                 validation_data=(x_valid, label_valid),\n                 epochs=50,\n                 batch_size=32768,\n                 callbacks=[lr_sched], \n                 verbose=1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "9cce9374cc184d75261577612d54f7231d8cec27"
      },
      "cell_type": "code",
      "source": "# Plot training & validation loss values\nimport matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation mae values\nplt.plot(history.history['mean_absolute_error'])\nplt.plot(history.history['val_mean_absolute_error'])\nplt.title('Mean Abosulte Error')\nplt.ylabel('Mean absolute error')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ccbeafd6be87521e9614892aff62a989ae969f6a"
      },
      "cell_type": "code",
      "source": "pred = model.predict(X_test)\npred = pred.ravel()\n\n# ori_test['winPlacePerc'] = np.clip(pred, a_min=0, a_max=1)\n\nprint(\"fix winPlacePerc\")\nfor i in range(len(ori_test)):\n    winPlacePerc = pred[i]\n    maxPlace = int(ori_test.iloc[i]['maxPlace'])\n    if maxPlace == 0:\n        winPlacePerc = 0.0\n    elif maxPlace == 1:\n        winPlacePerc = 1.0\n    else:\n        gap = 1.0 / (maxPlace - 1)\n        winPlacePerc = round(winPlacePerc / gap) * gap\n    \n    if winPlacePerc < 0: winPlacePerc = 0.0\n    if winPlacePerc > 1: winPlacePerc = 1.0    \n    pred[i] = winPlacePerc\n\n    #if (i + 1) % 100000 == 0:\n    #    print(i, flush=True, end=\" \")\n\n'''\ndf_test['winPlacePercPred'] = np.clip(pred, a_min=0, a_max=1)\n\n\naux = df_test.groupby(['matchId','groupId'])['winPlacePercPred'].agg('mean').groupby('matchId').rank(pct=True).reset_index()\naux.columns = ['matchId','groupId','winPlacePerc']\ndf_test = df_test.merge(aux, how='left', on=['matchId','groupId'])\n'''\nori_test['winPlacePerc'] = pred\nsubmission = ori_test[['Id', 'winPlacePerc']]\n\nsubmission.to_csv('submission.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "11fa07b29d8dcc78c5ba53379cae6d22175c53e0"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}