{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gc\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "Input = \"../input/\"\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "ori_data = pd.read_csv(Input + \"train_V2.csv\")\n",
    "ori_test = pd.read_csv(Input + \"test_V2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4979388e439d34a0919599cec0b50c9b96e3c455"
   },
   "outputs": [],
   "source": [
    "print (ori_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2226f99ca6149c478570e4e32866aec3215a7d4"
   },
   "outputs": [],
   "source": [
    "print (ori_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "761a2f3626bd049789350065b2d6f0d61df02b3d"
   },
   "outputs": [],
   "source": [
    "# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c2007c269d78ac981aa36ec51e406164e5d21ca7"
   },
   "outputs": [],
   "source": [
    "ori_data = reduce_mem_usage(ori_data)\n",
    "ori_test = reduce_mem_usage(ori_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b20e7e9c870107ad5f4b6c5bded9f5f03dd82efb"
   },
   "outputs": [],
   "source": [
    "# changed from https://www.kaggle.com/anycode/simple-nn-baseline\n",
    "# Adding features based on https://www.kaggle.com/powercode/sklearn-mlp\n",
    "def feature_engineering(is_train=True, df_train = ori_data, df_test = ori_test):\n",
    "    if is_train: \n",
    "        print(\"processing train.csv\")\n",
    "        df = df_train\n",
    "        df = df[df['maxPlace'] > 1]\n",
    "    else:\n",
    "        print(\"processing test.csv\")\n",
    "        df = df_test\n",
    "    \n",
    "    print (\"Adding some features\")\n",
    "    \n",
    "    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n",
    "    df[\"skill\"] = df[\"headshotKills\"] + df[\"roadKills\"]\n",
    "    df['healthitems'] = df['heals'] + df['boosts']\n",
    "    \n",
    "    '''\n",
    "    df['killPlace_over_maxPlace'] = df['killPlace'] / df['maxPlace']\n",
    "    df['headshotKills_over_kills'] = df['headshotKills'] / df['kills']\n",
    "    df['distance_over_weapons'] = df['totalDistance'] / df['weaponsAcquired']\n",
    "    df['walkDistance_over_heals'] = df['walkDistance'] / df['heals']\n",
    "    df['walkDistance_over_kills'] = df['walkDistance'] / df['kills']\n",
    "    df['killsPerWalkDistance'] = df['kills'] / df['walkDistance']\n",
    "    df['headshotrate'] = df['kills']/df['headshotKills']\n",
    "    df['killStreakrate'] = df['killStreaks']/df['kills']\n",
    "    \n",
    "    df[df == np.Inf] = np.NaN\n",
    "    df[df == np.NINF] = np.NaN\n",
    "    \n",
    "    print(\"Removing Na's from df\")\n",
    "    df.fillna(0, inplace=True)\n",
    "    '''\n",
    "    # df = df[:100]\n",
    "    \n",
    "    print(\"remove some columns\")\n",
    "    target = 'winPlacePerc'\n",
    "    features = list(df.columns)\n",
    "    features.remove(\"Id\")\n",
    "    features.remove(\"matchId\")\n",
    "    features.remove(\"groupId\")\n",
    "    features.remove(\"matchType\")\n",
    "    \n",
    "    # matchType = pd.get_dummies(df['matchType'])\n",
    "    # df = df.join(matchType)    \n",
    "    \n",
    "    y = None\n",
    "    \n",
    "    print(\"get target\")\n",
    "    if is_train: \n",
    "        y = np.array(df.groupby(['matchId','groupId'])[target].agg('mean'), dtype=np.float64)\n",
    "        features.remove(target)\n",
    "\n",
    "    print(\"get group mean feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    \n",
    "    if is_train: df_out = agg.reset_index()[['matchId','groupId']]\n",
    "    else: df_out = df[['matchId','groupId']]\n",
    "\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    print(\"get group max feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    print(\"get group min feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    print(\"get group size feature\")\n",
    "    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n",
    "    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    print(\"get match mean feature\")\n",
    "    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n",
    "    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n",
    "    \n",
    "    print(\"get match size feature\")\n",
    "    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n",
    "    df_out = df_out.merge(agg, how='left', on=['matchId'])\n",
    "    \n",
    "    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n",
    "\n",
    "    X = np.array(df_out, dtype=np.float64)\n",
    "    \n",
    "    feature_names = list(df_out.columns)\n",
    "\n",
    "    del df, df_out, agg, agg_rank\n",
    "    gc.collect()\n",
    "\n",
    "    return X, y, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b0e17888ebedefbce035cbd757ad66ad2a1d1ba"
   },
   "outputs": [],
   "source": [
    "X_train, y_train, feature_names = feature_engineering()\n",
    "print (len(feature_names))\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "18c812e6a69eabdfeb7785463bb306e2679256d6"
   },
   "outputs": [],
   "source": [
    "print (feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5570836b2d696220138af638ec153d7c1765f23c"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Input, Activation\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.models import load_model, Model\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "86814c22e98b66ff2763b893e6ec8f3d2210f7a5"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "model = Sequential()\n",
    "model.add(Dense(512, kernel_initializer='he_normal', input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(256, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "'''\n",
    "\n",
    "main_input = Input(shape=(X_train.shape[1],), name='main_input')\n",
    "x = Dense(1024, kernel_initializer='he_normal')(main_input)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "# got 0\n",
    "x = Dense(512, kernel_initializer='he_normal')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "temp_1 = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "# temp_1 = x\n",
    "# got 1\n",
    "x = Dense(512, kernel_initializer='he_normal')(temp_1)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "# got 2\n",
    "x = Dense(512, kernel_initializer='he_normal')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "# got 3\n",
    "x = Dense(512, kernel_initializer='he_normal')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "added_1 = keras.layers.Add()([temp_1,x])\n",
    "x = keras.layers.LeakyReLU(alpha=0.3)(added_1)\n",
    "# got 4\n",
    "x = Dense(256, kernel_initializer='he_normal')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "temp_2 = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "# temp_2 = x\n",
    "# got 5\n",
    "x = Dense(256, kernel_initializer='he_normal')(temp_2)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "# got 6\n",
    "x = Dense(256, kernel_initializer='he_normal')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "# got 7\n",
    "x = Dense(256, kernel_initializer='he_normal')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "added_2 = keras.layers.Add()([temp_2,x])\n",
    "x = keras.layers.LeakyReLU(alpha=0.3)(added_2)\n",
    "# got 8\n",
    "x = Dense(128, kernel_initializer='he_normal')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "temp_3 = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "#temp_3 = x\n",
    "# got 9\n",
    "x = Dense(128, kernel_initializer='he_normal')(temp_3)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "# got 10\n",
    "x = Dense(128, kernel_initializer='he_normal')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "# got 11\n",
    "x = Dense(128, kernel_initializer='he_normal')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "added_3 = keras.layers.Add()([temp_3,x])\n",
    "x = keras.layers.LeakyReLU(alpha=0.3)(added_3)\n",
    "# got 12\n",
    "x = Dense(64, kernel_initializer='he_normal')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "temp_4 = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "# got 13\n",
    "x = Dense(64, kernel_initializer='he_normal')(temp_4)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "# got 14\n",
    "x = Dense(64, kernel_initializer='he_normal')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "# got 15\n",
    "x = Dense(64, kernel_initializer='he_normal')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "added_4 = keras.layers.Add()([temp_4,x])\n",
    "x = keras.layers.LeakyReLU(alpha=0.3)(added_4)\n",
    "# got 16\n",
    "x = Dense(32, kernel_initializer='he_normal')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "# got 17\n",
    "x = Dense(16, kernel_initializer='he_normal')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "# got 18\n",
    "out = Dense(1, kernel_initializer='normal', activation='sigmoid')(x)\n",
    "# got output\n",
    "model = Model(inputs=main_input, outputs=out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b0f00bae9917ed0748b0abc77678dd1fab6080e8"
   },
   "outputs": [],
   "source": [
    "print (X_train.shape)\n",
    "print (y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1cb05d30e7a5504512ba1c61d0944202a9e9410c"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "x_train, x_valid, label_train, label_valid = train_test_split(X_train, y_train, test_size = 0.2, random_state = 1234)\n",
    "del X_train, y_train\n",
    "gc.collect()\n",
    "print (\"Finishing Split!\")\n",
    "# scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(x_train)\n",
    "scaler = preprocessing.QuantileTransformer().fit(x_train)\n",
    "\n",
    "x_train = scaler.transform(x_train)\n",
    "x_valid = scaler.transform(x_valid)\n",
    "X_test, _, _ = feature_engineering(is_train = False)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"x_train\", x_train.shape, x_train.min(), x_train.max())\n",
    "print(\"x_valid\", x_valid.shape, x_valid.min(), x_valid.max())\n",
    "print(\"X_test\", X_test.shape, X_test.min(), X_test.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "618ef68b8f93f4445a7e09086a1bcc79dc5b0d62"
   },
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(lr=0.01, epsilon=1e-8, decay=1e-4, amsgrad=True)\n",
    "# optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10, verbose=0):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
    "    '''\n",
    "    def schedule(epoch):\n",
    "        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
    "    \n",
    "    return LearningRateScheduler(schedule, verbose)\n",
    "\n",
    "lr_sched = step_decay_schedule(initial_lr=0.1, decay_factor=0.9, step_size=1, verbose=1)\n",
    "# early_stopping = EarlyStopping(monitor='val_mean_absolute_error', min_delta=0, mode = 'min', patience=8, verbose=1)\n",
    "\n",
    "history = model.fit(x_train, label_train, \n",
    "                 validation_data=(x_valid, label_valid),\n",
    "                 epochs=50,\n",
    "                 batch_size=32768,\n",
    "                 callbacks=[lr_sched], \n",
    "                 verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9cce9374cc184d75261577612d54f7231d8cec27",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation mae values\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['val_mean_absolute_error'])\n",
    "plt.title('Mean Abosulte Error')\n",
    "plt.ylabel('Mean absolute error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ccbeafd6be87521e9614892aff62a989ae969f6a"
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred = pred.ravel()\n",
    "\n",
    "# ori_test['winPlacePerc'] = np.clip(pred, a_min=0, a_max=1)\n",
    "\n",
    "print(\"fix winPlacePerc\")\n",
    "for i in range(len(ori_test)):\n",
    "    winPlacePerc = pred[i]\n",
    "    maxPlace = int(ori_test.iloc[i]['maxPlace'])\n",
    "    if maxPlace == 0:\n",
    "        winPlacePerc = 0.0\n",
    "    elif maxPlace == 1:\n",
    "        winPlacePerc = 1.0\n",
    "    else:\n",
    "        gap = 1.0 / (maxPlace - 1)\n",
    "        winPlacePerc = round(winPlacePerc / gap) * gap\n",
    "    \n",
    "    if winPlacePerc < 0: winPlacePerc = 0.0\n",
    "    if winPlacePerc > 1: winPlacePerc = 1.0    \n",
    "    pred[i] = winPlacePerc\n",
    "\n",
    "    #if (i + 1) % 100000 == 0:\n",
    "    #    print(i, flush=True, end=\" \")\n",
    "\n",
    "'''\n",
    "df_test['winPlacePercPred'] = np.clip(pred, a_min=0, a_max=1)\n",
    "\n",
    "\n",
    "aux = df_test.groupby(['matchId','groupId'])['winPlacePercPred'].agg('mean').groupby('matchId').rank(pct=True).reset_index()\n",
    "aux.columns = ['matchId','groupId','winPlacePerc']\n",
    "df_test = df_test.merge(aux, how='left', on=['matchId','groupId'])\n",
    "'''\n",
    "ori_test['winPlacePerc'] = pred\n",
    "submission = ori_test[['Id', 'winPlacePerc']]\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "11fa07b29d8dcc78c5ba53379cae6d22175c53e0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
