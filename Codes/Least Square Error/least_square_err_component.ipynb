{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from autograd import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom autograd import grad\nimport matplotlib.pyplot as plt\n\"\"\"\nchanged from https://www.kaggle.com/anycode/simple-nn-baseline\n\nto run this kernel, pip install ultimate first from your custom packages\n\"\"\"\n\nimport gc, sys\n\ngc.enable()\n\nINPUT_DIR = \"../input/\"\n\n\ndef gradient_descent(g, epochs, alpha, bealta, w, batch_size):\n    # compute gradient module using autograd\n    gradient = grad(g)\n    # d = 0\n\n    #steps = x.shape[0] / batch_size\n    #steps = int(steps)\n\n    steps = int(np.ceil(x.shape[0] / batch_size))\n\n    # run the gradient descent loop\n    weight_history = [w]  # weight history container\n    cost_history = [g(w, range(0, x.shape[0]))]  # cost function history container\n    for j in range(epochs):\n        print('epoch:' + str(j+1) + '  cost:'+ str(cost_history[-1]))\n        for i in range(steps):\n            batch = np.arange(i * batch_size, min((i + 1) * batch_size, x.shape[0]))\n            \n            # take gradient descent step\n            grad_eval = gradient(w, (batch))\n            # normalized_term = np.linalg.norm(grad_eval)\n\n            grad_eval = grad_eval / (np.abs(grad_eval) + 10**(-8))\n\n            w = w - alpha * grad_eval\n\n            # w = w - (alpha/(normalized_term+10**(-16))) * grad_eval \n\n            # record weight and cost\n        weight_history.append(w)\n        cost_history.append(g(w, range(0, x.shape[0])))\n    return weight_history, cost_history\n\ndef model(x_p, w):\n    a = w[0] + np.dot((x_p), w[1:])\n    return a.T\ndef cost_function(w,iter):\n    x_p = x[iter,:]\n    y_p = y[iter]\n    cost = np.sum((model(x_p, w) - y_p)**2)\n    # cost = np.sum(np.absolute(model(x_p, w)-y_p))\n    #cost = np.sum(np.log(1 + np.exp(-y_p * model(x_p, w))))\n    return cost/float(np.size(y_p))\n\n\ndef feature_engineering(is_train=True):\n    if is_train:\n        print(\"processing train.csv\")\n        #df = pd.read_csv(INPUT_DIR + 'train_V2.csv', nrows = 10000)\n        df = pd.read_csv(INPUT_DIR + 'train_V2.csv')\n        df = df[df['maxPlace'] > 1]\n    else:\n        print(\"processing test.csv\")\n        #df = pd.read_csv(INPUT_DIR + 'test_V2.csv', nrows = 10000)\n        df = pd.read_csv(INPUT_DIR + 'test_V2.csv')\n    # df = reduce_mem_usage(df)\n    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n\n\n\n    print(\"remove some columns\")\n    target = 'winPlacePerc'\n    features = list(df.columns)\n    features.remove(\"Id\")\n    features.remove(\"matchId\")\n    features.remove(\"groupId\")\n\n    features.remove(\"matchType\")\n\n\n    y = None\n\n    print(\"get target\")\n    if is_train:\n        y = np.array(df.groupby(['matchId', 'groupId'])[target].agg('mean'), dtype=np.float64)\n        features.remove(target)\n\n    print(\"get group mean feature\")\n    agg = df.groupby(['matchId', 'groupId'])[features].agg('mean')\n    #agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n\n    if is_train:\n        df_out = agg.reset_index()[['matchId', 'groupId']]\n    else:\n        df_out = df[['matchId', 'groupId']]\n\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n   # df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n\n\n    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n\n    X = np.array(df_out, dtype=np.float64)\n\n    feature_names = list(df_out.columns)\n\n    del df, df_out, agg\n    gc.collect()\n\n    return X, y, feature_names\n\n\ndata_x, y, feature_names = feature_engineering(True)\n#y = y * 2 - 1\nw0 = 1 * np.random.rand(data_x.shape[1]+ 1, 1)\n\nx_means = np.mean(data_x,axis = 0)[np.newaxis,:]\nx_stds = np.std(data_x,axis = 0)[np.newaxis,:]\nx = (data_x - x_means)/(x_stds+0.0000001)\n\nw_history, cost_history = gradient_descent(cost_function, 200, 0.01, 0.95, w0, 32768)\n\nw1 = w_history[-1]\nprint (cost_history[-1])\n\nplt.plot(cost_history)\nplt.show()\nx_test, _, _ = feature_engineering(False)\nx_means = np.mean(x_test,axis = 0)[np.newaxis,:]\nx_stds = np.std(x_test,axis = 0)[np.newaxis,:]\nx_test = (x_test - x_means)/(x_stds+0.0000001)\nprint(x_test.shape)\n\ny_predict =  model(x_test, w1)\ny_predict = y_predict.reshape(-1,1)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(y_predict)\ny_predict = scaler.transform(y_predict)\n\n\ndf_test = pd.read_csv(INPUT_DIR + 'test_V2.csv')\n\ndf_test['winPlacePerc'] = y_predict\n\nfor i in range(len(df_test)):\n    winPlacePerc = y_predict[i][0]\n    maxPlace = int(df_test.iloc[i]['maxPlace'])\n    if maxPlace == 0:\n        winPlacePerc = 0.0\n    elif maxPlace == 1:\n        winPlacePerc = 1.0\n    else:\n        gap = 1.0 / (maxPlace - 1)\n        winPlacePerc = round(winPlacePerc / gap) * gap\n\n    if winPlacePerc < 0: winPlacePerc = 0.0\n    if winPlacePerc > 1: winPlacePerc = 1.0\n    y_predict[i][0] = winPlacePerc\n\na = np.sum(y_predict)/float(len(y_predict))\n#y_predict\ndf_test['winPlacePerc'] = y_predict\nsubmission = df_test[['Id', 'winPlacePerc']]\nsubmission.to_csv('Abscomponentall.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}