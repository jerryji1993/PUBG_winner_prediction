{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction  \n",
    "Player Unknown’s Battle Ground (PUBG) is a top-selling online shooting and survival game, in which around 100 players will be eliminating each other and striving to survive as long as possible until there is only one player standing, who will be the final winner. In this project, we used around 4,446,960 anonymized players’ data that consists of a label vector of final placement percentile, and a feature matrix based on different player ratings and final in-game stats to build models. Specifically, we trained independent models based on cutting-edge machine learning and deep learning algorithms (ML: XGBoost/LightGBM; DL: neural networks like ResNets). We compared the performance of these advanced models with the models introduced in the class (Linear regression, logistic regression, etc.). At the same time, we also compared the performance of different optimization methods, such as Adam, RMSprop, and various versions of the gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline models  \n",
    "In this section, we will introduce baseline models we build to solve the PUBG problem. It is obvious that this problem is a regression problem since the output label is the continuous value from 0 to 1 (0 for the last placement, 1 for the first placement). It is straightforward for us to utilize regression models to predict the final placement. First, we tried to use the least square error with five different optimizers to fit the data and conduct the prediction. These five optimizers are the gradient descent with momentum, gradient descent with normalization, gradient descent with component-wise normalization, Adam, and RMSprop. The first three optimizers had been fully introduced during the lectures. Adam and the RMSprop combine the idea of normalized gradient descent with the momentum thoughts. The implementation of Adam and RMSprop is similar with that of the general gradient descent. The main difference is the rule of updating weights, which is given in the following equations (more details can be found in our codes). \n",
    "\n",
    "After training the model we constructed, we obtained the optimal weight matrix and bias. With such matrix and bias, we can calculate the predictions of the final placement percentile through the equation $Pred = \\mathbf{W} * \\mathbf{x} + \\mathbf{b}$, and normalize these predictions to the range between 0 and 1.  \n",
    "\n",
    "The first row of Table 1 presents the Mean Absolute Error results on the test set (around 1,930,000 players) for the least square error based models with different optimizers. From this row, we can see that except the component-wise optimizer, all the remaining four optimizers give us the similar performance on the test set. Among all of them, the momentum optimizer tends to obtain the best weights and perform the best predictions on the test set. Moreover, Adam, RMSprop, and normalized gradient descent optimizers also can provide good results on this problem. And the reasons why the component-wise optimizer performs worst might be that this optimizer is more sensitive to the initialization settings and not robust enough. To summarize, the least square error based models can provide some reasonable results on the PUBG problem.  \n",
    "\n",
    "Then we also built absolute error based models regarding these 5 different optimizers. Through training and testing, we obtained the corresponding results shown in the second row of Table 1. From this row, we can see that RMSprop and Momentum optimizers can give us some good results on the test set. However, the remaining three optimizers only achieve the MAE around 0.2500, which is bad to some extent. Moreover, Component-wise optimizer still performs the worst, which shows that it is really hard to optimize such kind of problem with this optimizer. In addition, Adam also performs bad and the reason might be related to the hyperparameter settings (Adam has more parameters to tune). In conclusion, Absolute error based models are not appropriate for such kind of problem and the reason might be that the data of this problem is not sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Normalized Gradient Descent | Component-wise Gradient Descent | Momentum | Adam | RMSprop |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Least Square Error | 0.2068 | 0.2406 | $\\textbf{0.2027}$ | 0.2040 | 0.2050 |\n",
    "| Absolute Error | 0.2499 | 0.2529 | 0.2303 | 0.2505 | $\\textbf{0.2291}$ |\n",
    "| Softmax Cost | $\\textbf{0.1961}$ | 0.1995 | 0.1989 | 0.1990 | 0.1989 |\n",
    "| Perceptron Cost | 0.2356 | 0.2487 | 0.2421 | 0.2269 | $\\textbf{0.2057}$ |  \n",
    "<strong>Table 1:</strong> <em> Mean Absolute Error on the test set for all the baselines we write from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the baselines, we also tried to use the softmax cost function and perceptron cost function to conduct the optimization and model the data. In addition, we applied 5 different optimizers to optimize these two cost functions hoping to find one optimizer can make our model achieve lower Mean Absolute Error on the test set.  \n",
    "\n",
    "It is obvious that the softmax cost and perceptron cost functions are always used to solve the two-class classification problem. However, our problem tends to be like a regression problem because its label data in the training set is continuous representing a percentile winning placement. It is sure that if we directly apply these two functions to the problem, we tend to obtain bad results. In order to fit the problem, we found a trick to transfer the original problem to a two-class classification problem. As we all know, the label data represents the percentile winning placement. To some extent, such kind of label can be seen as the winning probabilities (being the 1st place in the battle) of players. If the label is closer to 1, then the winning probability (being the 1st place in the battle) is closer to 1. Following such kind of idea, we set player labels to $\\textbf{-1}$ if their percentile winning placement was lower than 0.5 because we thought these players tended not to be the 1st place in the battle. On the other hand, we set player labels to $\\textbf{1}$ if their percentile winning placement was greater or equal to 0.5 because we thought they tended to be the 1st place in the battle. Then, we had transferred our original regression problem to a two-class classification problem (whether they can be the 1st place in the competition).  \n",
    "\n",
    "After training, we calculated the predictions of winning possibilities through the equation $Pred = \\mathbf{W} * \\mathbf{x} + \\mathbf{b}$, and normalize these predictions to the range between 0 and 1 (converting to the winning possibility).  \n",
    "\n",
    "The third row of Table 1 presents the Mean Absolute Error results on the test set for the softmax-models with different optimizers. From this row, we can see that these five optimizers give us the similar performance on the test set. Among all of them, the gradient descent with normalization tends to obtain the best weights and perform the best predictions on the test set. Moreover, Adam, RMSprop, and Momentum optimizers also can provide good results on this problem. In addition, the reasons why the component-wise optimizer performs worst might be two-fold. The first reason might be that this optimizer tends to fall into some local minimum easily. The second reason tends to be related to the weight initialization. To summarize, the optimizers do not have a great impact on the performance of softmax based models.  \n",
    "\n",
    "The last row of Table 1 shows the Mean Absolute Error results on the test set for the perceptron-models with different optimizers. It is obvious that RMSprop can achieve the lowest MAE. In addition, Adam also performs better than Momentum, Component-wise, and normalized gradient descent methods do. These results show the high efficiency of RMSprop and Adam which we wrote from scratch.  \n",
    "\n",
    "Besides training the models we wrote from scratch (all of the models mentioned above), we also tried to use Scikit-learn [1] packages to construct models and conduct the predictions on the test set. First, we tried to use 'liner_model.Ridge' and 'linear_model.LogisticRegression' to construct models and provide predictions. For the 'liner_model.Ridge'(same as Least Square Error), we obtained the MAE (Mean Absolute Error) of $\\textbf{0.2039}$ on the test set, which was similar to what we obtained from the least square error written by ourselves. In addition, we achieved the MAE (Mean Absolute Error) of $\\textbf{0.1778}$ (our logistic's best result is 0.1961) for the 'liner_model.LogisticRegression'. It seems that sklearn packages work better than what we wrote from scratch, especially for the logistic function. Moreover, it took less time ($\\textbf{20x}$, around 1-2 mins) to train sklearn models than the models we wrote from scratch (20-40 mins). All of these show that sometimes reinventing the wheel is not a good idea.  \n",
    "\n",
    "After trying the 'liner_model.Ridge' and 'linear_model.LogisticRegression', we also tried to use 'DecisionTreeRegressor' and 'RandomForestRegressor' to construct models, optimize, and conduct predictions. Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. The deeper the tree, the more complex the decision rules, and the fitter the model.[2] However, the overfitting problem might occur if we build a large and deep decision tree. In order to prevent the overfitting problem, we utilized RandomForestRegressor to construct and train models. RandomForestRegressor does this by creating random subsets of the features and building smaller (shallow) trees using the subsets and then it combines the subtrees. The downside of RandomForestRegressor is that it can be slow if you have a single process but it can be parallelized to speed up the process. After conducting experiments, we obtained the MAE (Mean Absolute Error) of $\\textbf{0.0921}$ on the test set. Moreover, RandomForestRegressor helped us achieve the highest score (its MAE (Mean Absolute Error) is $\\textbf{0.0711}$) among all the baseline models. The following plot presents the performance of different baselines mentioned above, where 'LSE_sklearn' means Least Square Error with sklearn packages.  \n",
    "\n",
    "![](images/BaselinePerformance.png)  \n",
    "<strong>Figure 1:</strong> <em> The performance of baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks  \n",
    "Besides the baselines we mentioned above, we also explored other ways to solve the PUBG problem, such as deep neural networks, ResNet, LightGBM, and XGBoost.  \n",
    "\n",
    "A deep neural network is an artificial neural network with multiple layers between the input and output layers [3][4]. It can find a linear or a non-linear relationship between the input and output. Generally, the network moves through the layers calculating the probability of each output. For example, we can train a DNN (deep neural network) to recognize the kind of object in the image. The DNN will go over the input image, extract different features with different layer mathematical manipulations, and calculate the probability that the object in the image is a certain kind. Then, we can select the highest probability and return its corresponding label as the kind of that object in the image.  \n",
    "\n",
    "DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and returned as the output. If the network didn’t accurately recognize a particular pattern, an algorithm like Backpropagation would adjust the weights. That can make certain parameters more influential until it determines the correct mathematical manipulation to fully process the data. [5] There are many variants for deep neural networks, such as Recurrent neural networks [6] in the natural language processing and convolutional neural networks [7] in the computer vision.  \n",
    "\n",
    "To solve this PUBG problem, extract efficient features, and find the relationship between the input (the competition stats) and output (percentile winning placement), we build a 16-layer (without the input and output layers) neural network with BachNormalization [8] and leakyReLU [9] activation function. In addition, the output layer utilizes one neuron with the sigmoid activation function to predict the percentile winning placement. The architecture of this neural network can be seen in the following picture.  \n",
    "\n",
    "![](images/DNN1.png)  \n",
    "<strong>Figure 2:</strong> <em> The architecture of DNN-model  \n",
    "\n",
    "We set the initial learning rate is 0.1 and the learning rate decay factor is 0.9. Moreover, we initialized all the weights (except the weights for the output layer, we set this layer's weight based on the normal distribution) related to this DNN by 'he_normal' [10]. And we utilized Mean Squared Error as the loss and Mean Absolute Error as the metric. In addition, we split the train set into a training set (0.8 * train set) and a validation set (0.2 * train set). Then, we used Adam with the mini-batch = 32768 to train this DNN and conduct the cross-validation on our training set and validation set. The following left figure shows the model loss on the training set and validation set within 50 epochs. It is clear that both the training loss and the test loss decrease to the value near to 0. And the following right figure shows the metric on the training set and validation set within 50 epochs. We can see the similar trends (the training metric and validation metric reduce to some low value around 0.03) in this plot. Finally, we obtained the MAE of $\\textbf{0.0293}$ on the test set.  \n",
    "\n",
    "![](images/DNNResult.png)  \n",
    "<strong>Figure 3:</strong> <em> DNN-model: Model Loss and Model Metric on the training set and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet\n",
    "As with artificial neural networks, many issues can arise with naively trained DNNs. One problem is the degradation problem. With the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error [11]. In order to alleviate such kind of problem, Kaiming He et al. proposed ResNet to construct deeper neural networks, extract more efficient features and obtain higher training and testing accuracy. The main idea of ResNet is the use of residual connections, which is also known as the residual learning with identity mapping by shortcuts.  \n",
    "\n",
    "In order to gain better results and extract more efficient features for the PUBG problem, we decide to construct deeper networks with the help of ResNet. The following figure is the architecture of our model, the model is a 19-layer (without the input and out layer) neural network with 4 residual learning blocks. The remaining settings of the model are the same as that of the DNN model mentioned above.  \n",
    "\n",
    "![](images/ResNet1.png)  \n",
    "<strong>Figure 4:</strong> <em> The architecture of ResNetBasedMode  \n",
    "\n",
    "As what we mentioned in the training process of the DNN model, we still used the same initial learning rate, decay factor, and weight initialization method. And we still utilized Mean Squared Error as the loss and Mean Absolute Error as the metric. In addition, we split the train set into a training set (0.8 * train set) and a validation set (0.2 * train set). Then, we used Adam with the mini-batch = 32768 to train this DNN and conduct the cross-validation on our training set and validation set. The following left figure shows the model loss on the training set and validation set within 50 epochs. It is clear that both the training loss and the test loss decrease to the value near to 0. And the following right figure shows the metric on the training set and validation set within 50 epochs. We can see the similar trends (the training metric and validation metric reduce to some low value around 0.025) in this plot. Finally, we obtained the MAE of $\\textbf{0.0226}$ on the test set.  \n",
    "\n",
    "![](images/ResNetResult.png)  \n",
    "<strong>Figure 5:</strong> <em> ResNetBasedMode: Model Loss and Model Metric on the training set and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM and XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we aim at exploring the use of some advanced nonlinear models to enhance our model performance and final predictions. In particular, this section focuses on tree-based ensemble models that apply boosting technique. We will be using two recent implementations of Gradient Boosted Decision Tree (GBDT) algorithm: LightGBM and XGBoost. Both of them are currently very popular in machine learning and data science field, as it is estimated that winners of more than half of competitions on Kaggle used one of the two frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by introducing the some background about GBDT algorithm and these two specific implementations first. In class, recall that Jeremy introduced boosting algorithm as sequentially (one at a time) adding non-linear features from the set $\\mathcal{F} = \\{f_{1}\\left(\\mathbf{x}\\right),\\,f_{2}\\left(\\mathbf{x}\\right),\\,\\ldots,\\,f_B\\left(\\mathbf{x}\\right)\\}$ that belongs to the same class of universal approximators to build the model:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{model}\\left(\\mathbf{x},\\Theta\\right) = w_0 + f_{s_1}\\left(\\mathbf{x}\\right){w}_{1} +  f_{s_2}\\left(\\mathbf{x}\\right){w}_{2} + \\cdots + f_{s_B}\\left(\\mathbf{x}\\right)w_{B} = w_0 + \\sum_{b=1}^{B}f_{s_b}\\left(\\mathbf{x}\\right)w_{b}\n",
    "\\end{equation}\n",
    "\n",
    "where $f_{s_k}$ denotes the unit added at the $k^{th}$ round of this process. Here, we specifically restrict our family of universal approximators to be **decision trees**. The boosting algorithm will run for total $B+1$ rounds with the individual summand that best minimizes our training error being added to the model, starting from the bias $w_0$. We evaluate our model and determine the best number of $k$ (i.e. how many total boosting rounds should be performed, $k^*$) based on validation error. We can, therefore, set a criterion of $m$ boosting rounds, and **early stop** if the validation error does not improve over $m$ rounds.\n",
    "\n",
    "The cost function we will apply in this project is **Mean Absolute Error** (MAE) as this is used as the final evaluation metric for the Kaggle competition:\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(\\Theta_M^{\\,}\\right) = \\frac{1}{P}\\sum_{p=1}^{P}\\left|\\text{model}_M^{\\,}\\left(\\mathbf{x}_p,\\Theta_M^{\\,}\\right) - y_p\\right|\n",
    "\\end{equation}\n",
    "\n",
    "If we view our model in a recursive manner, the model becomes\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{model}_M^{\\,}\\left(\\mathbf{x}_p^{\\,},\\Theta_M^{\\,}\\right) =   \\text{model}_{M-1}^{\\,}\\left(\\mathbf{x}_p^{\\,},\\Theta_{M-1}^{\\,}\\right) + f_M^{\\,}\\left(\\mathbf{x}_p\\right)w_M^{\\,}\n",
    "\\end{equation}\n",
    "\n",
    "And our \"iterative form\" of cost function becomes\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(\\Theta_M^{\\,}\\right) = \\frac{1}{P}\\sum_{p=1}^{P}\\left|f_M^{\\,}\\left(\\mathbf{x}_p^{\\,}\\right)w_M^{\\,} - \\left(y_p^{\\,} - \\text{model}_{M-1}^{\\,}\\left(\\mathbf{x}_p^{\\,}\\right)\\right)\\right|\n",
    "\\end{equation}\n",
    "\n",
    "Note that the $y_p^{\\,} - \\text{model}_{M-1}^{\\,}(\\mathbf{x}_p^{\\,})$ on RHS is essentially just the **residuals** of the previous model, i.e. the \"leftover\" part that is yet to be learned by the updated model. We denote this by $r_{M-1,\\ p}$. Effectively, minimizing the cost function now becomes minimizing the absolute difference between the newly added summand and the residual of previous model i.e. we are \"fitting to the residual\". That means, our ideal new summand $s_M^{*\\,} = f_M^{\\,}\\left(\\mathbf{x}_p,\\mathbf{v}_M^{*\\,}\\right)w_M^{*\\,}$ should be the one that minimizes our new cost function $g\\left(\\Theta_M^{\\,}\\right)$, where $\\mathbf{v}_M^{*\\,}$ is the tuned internal parameters of $f_M^{\\,}\\left(\\mathbf{x}_p\\right)$. i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "(w_M^{*\\,},\\mathbf{v}_M^{*\\,}) = \\underset{w_M^{\\,},\\mathbf{v}_M^{\\,}}{\\text{argmin}} \\quad \\frac{1}{P}\\sum_{p=1}^{P}|(\\text{model}_{M-1}^{\\,}(\\mathbf{x}_p^{\\,}) - y_p^{\\,}) + f_M^{\\,}(\\mathbf{x}_p^{\\,},\\mathbf{v}_M^{\\,})w_M^{\\,} |\n",
    "\\end{equation}\n",
    "\n",
    "However, direct computation of these optimal parameters could be difficult. Let's get back to our cost function:\n",
    "\n",
    "\\begin{align}\n",
    "g\\left(\\Theta_M^{\\,}\\right) = \\frac{1}{P}\\sum_{p=1}^{P}|(\\text{model}_{M-1}^{\\,}(\\mathbf{x}_p^{\\,}) - y_p^{\\,}) + f_M^{\\,}(\\mathbf{x}_p^{\\,},\\mathbf{v}_M^{\\,})w_M^{\\,} | \\\\\n",
    "= \\frac{1}{P}\\sum_{p=1}^{P}|(\\text{model}_{M-1}^{\\,}(\\mathbf{x}_p^{\\,}) - y_p^{\\,})| + \\frac{1}{P}\\sum_{p=1}^{P}|f_M^{\\,}(\\mathbf{x}_p^{\\,},\\mathbf{v}_M^{\\,})w_M^{\\,} | \\\\\n",
    "= g\\left(\\Theta_{M-1}^{\\,}\\right) + \\frac{1}{P}\\sum_{p=1}^{P}|f_M^{\\,}(\\mathbf{x}_p^{\\,},\\mathbf{v}_M^{\\,})w_M^{\\,} |\n",
    "\\end{align}\n",
    "\n",
    "Note that this form is very similar to our gradient descent step: $\\mathbf{w}^{\\,k} = \\mathbf{w}^{\\,k-1} - \\alpha \\nabla g\\left(\\mathbf{w}^{k-1}\\right)$. We already know that at certain step $k$, the gradient descent direction is the one that is steepest i.e. best minimizes our cost. We also know that conservatively, we could determine the best $\\alpha$ that minimizes our function along the descent direction by line search. Therefore, **ideally, we want to make $f_M^{\\,}(\\mathbf{x}_p^{\\,},\\mathbf{v}_M^{\\,})w_M^{\\,}$ the \"largest step as possible on the correct direction\"**, which means, ideally, **$f_M^{\\,}(\\mathbf{x}_p^{\\,},\\mathbf{v}_M^{\\,})$ should be equal to the negative gradient of $g\\left(\\Theta_{M-1}^{\\,}\\right)$, while $w_M^{\\,}$ should be the ideal $\\alpha$**.\n",
    "\n",
    "Since we have two additional desires, we then have two additional costs. Following the derivation of the original published paper (Friedman, 1999, https://statweb.stanford.edu/~jhf/ftp/trebst.pdf), we have:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{v}_M^{*\\,} = \\underset{\\mathbf{v}, \\omega^{\\,}}{\\text{argmin}} \\quad \\sum_{p=1}^{P} [-[\\frac{\\partial g(y_p^{\\,}, \\text{model}_{M-1}(\\mathbf{x}_p))}{\\partial \\text{model}_{M-1}(\\mathbf{x}_p)}]\\ -\\ f_M^{\\,}(\\mathbf{x}_p^{\\,},\\mathbf{v}^{\\,})\\omega]^2\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "w_M^{*\\,} = \\underset{w^{\\,}}{\\text{argmin}} \\quad \\frac{1}{P}\\sum_{p=1}^{P}\\left|f_M^{\\,}(\\mathbf{x}_p^{\\,},\\mathbf{v}_M^{*\\,})w^{\\,} - \\left(y_p^{\\,} - \\text{model}_{M-1}^{\\,}\\left(\\mathbf{x}_p^{\\,}\\right)\\right)\\right|\n",
    "\\end{equation}\n",
    "\n",
    "The second parameter $w_M$ is determined exactly by line search. Our final model therefore becomes\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{model}_M^{\\,}(\\mathbf{x}_p^{\\,},\\Theta_M^{\\,}) =   \\text{model}_{M-1}^{\\,}(\\mathbf{x}_p^{\\,},\\Theta_{M-1}^{\\,}) + f_M^{\\,}(\\mathbf{x}_p, \\mathbf{v}_M^{*\\,})w_M^{*\\,}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost  \n",
    "Now that we have a basic idea about general gradient boosting algorithm, we can talk about XGBoost [12] and how it implements and improves the algorithm such that it becomes more accurate, efficient and flexible. Specifically, XGBoost implemented several important features below that makes the model perform better:\n",
    "\n",
    "-  It took also the **second order Taylor expansion** of the cost function in addition to the first order gradient, such that the $k$-th round cost function, after removal of constants, simplifies to only include the first and second order derivative of the previous cost function and the new summands. This would make it work and converge faster for any **convex** cost function (allowing user-defined convex cost functions also).\n",
    "\n",
    "\n",
    "\n",
    "-  It combines boosting with **regularization** by adding a regularization term to the cost function that controls model complexity by limiting the number of leaf nodes (pruning) and smoothing leaf values (L1/L2 regularization on leaf weights/scores). **This effectively ensures an additional layer in preventing the overfitting problem**.\n",
    "\n",
    "\n",
    "\n",
    "-  It allows additional techniques to prevent overfitting: e.g. explicit limitations on **tree depth**, **early stopping** of boosting, **shrinkage method** (Friedman, 2002), and row and **column subsampling** during model training.\n",
    "\n",
    "\n",
    "\n",
    "-  In addition to the traditional greedy search method to find the exact split points, XGBoost also incorporated an **approximate algorithm** to find split points when 1) data does not fit entirely to memory and 2) distributed setting. The innovation on this is two-fold: 1) **Weighted Quantile Sketch**: XGBoost incorporated a unique distributed weighted quantile sketch algorithm that can handle weighted data, instead of traditionally assuming every instance having equal weights; 2) **Sparsity-aware Split Finding**: XGBoost implemented a unified way of handling sparse data (e.g. missing values, one-hot-encoding, etc.) during the split finding process.\n",
    "\n",
    "\n",
    "\n",
    "-  Other system-level optimization to ensure better efficiency, including **block structure for parallel learning**, **cache awareness** and **out-of-core computing**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM  \n",
    "XGBoost is already a very optimized GBDT implementation based on the new features above that they incorporated into their model to improve the traditional framework. However, it still has some drawbacks. One of the limitations is that XGBoost, like many other GBDT tools, utilizes the \"pre-sort-based\" algorithm. Essentially, this means we pre-sort all the features and find the best splitting point with time complexity O(#data). This enables us to accurately find the best splitting points. However, the problem arises when **our dataset gets very large**. The first constraint is in terms of memory usage, as we not only need to store the original data as well as the pre-sorted feature values. Secondly, we need to calculate the gain for each and every split, resulting in the cost of time complexity O(#data) and lower speed. Here we introduce **LightGBM** [13] model, where major optimization is made to speed and memory usage by the following three major conceptual improvements:\n",
    "\n",
    "-  **Histogram-based algorithm**: Instead of pre-sort-based algorithms, LightGBM uses histogram-based algorithms for calculating the gain of each split (Recall what Jeremy has mentioned in class). Essentially the data in each feature is discretized into k integers and a histogram of k bins is constructed, and the **time complexity will be reduced greatly to O(#bins)**. The **memory usage is approximately reduced to 1/8** as the feature bins could be stored in int_8, while int_32 and float_32 are used to store sorted index and feature values in pre-sorted algorithms. Although this would result in less accuracy in finding the split points for individual trees as the features were discretized, this **functions as regularization to effectively prevent overfitting, and the final boosted tree often performs better**.\n",
    "\n",
    "\n",
    "\n",
    "-  **Histogram subtraction for further speed-up**: Each leaf's histogram in a binary tree equals the subtraction of histograms from its \"parent\" and its \"neighbor\". The speed of obtaining histograms could, therefore, be doubled.\n",
    "\n",
    "\n",
    "\n",
    "-  **Leaf-wise tree growth vs. Level-wise tree growth**: Traditional GBDT algorithms (including XGBoost) grow trees level-wise i.e. all leaves on one level will be split simultaneously. This is often unnecessary as the information gain from splitting on many leaves on the same level is low. In contrast, LightGBM uses leaf-wise growth where only the leaf with maximum delta loss will be grown on each level which tends to result in the lower final loss. This indeed may cause overfitting, so the max depth of tree in LightGBM is an important parameter to tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and hyperparameter tuning  \n",
    "We performed hyperparameter tuning for XGBoost and LightGBM model with **5-fold cross-validation**. For XGBoost, we tried to compare our learning rate = 0.05 or 0.1, maximum tree depth = 5, 10 or 15, and fraction of row and column subsampling = 0.7 or 0.8. Since our dataset is very large, we tried to set our total boosting rounds to be 20,000 or 30,000. We also set the early stopping rounds to be 100, such that we early terminate our boosting if the evaluation metric for the validation set did not improve over 100 rounds. However, it turned out that the validation error continues to drop throughout the full 30,000 boosting rounds in most cases. Final MAE on the test set using XGBoost was 0.0239."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/XGBoost2.png)  \n",
    "<strong>Figure 6:</strong> <em> XGBoost: 5-fold Cross-Validation Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LightGBM, since it grow trees based on leaf-wise strategy (as mentioned earlier) number of leaves per tree is an important hyperparameter to tune for preventing overfitting (default = 31). We tried to set this parameter to be n = 31, 40, 60 and 100, and it turns out that n = 100 gives best cross-validation error. We also set our learning rate = 0.05 or 0.1, fraction of row and column subsampling = 0.7 or 0.8. Same as in XGBoost total boosting rounds = 30,000 gives us better validation error. The final best parameter set determined after hyperparameter tuning is as below: {'objective': 'regression', 'num_threads': 60, 'num_leaves': 100, 'num_iterations': 30000, 'metric': 'mae', 'learning_rate': 0.05, 'early_stopping_round': 200, 'colsample_bytree': 0.7, 'boosting': 'gbdt', 'bagging_seed': 0, 'bagging_fraction': 0.8, 'optimal_number_of_trees': 30000}. This give us best MAE = 0.0204, placed us the **top 3% in this Kaggle competition**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/LightGBM2.png)  \n",
    "<strong>Figure 7:</strong> <em> LightGBM: 5-fold Cross-Validation Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also tried to apply **PCA sphereing** on our dataset before training XGBoost and LightGBM models. In contrast to our expectation, PCA sphereing did not improve our results as for LightGBM it gave us validation MAE = 0.03233 with all parameters kept the same (for XGBoost we got 0.03577). The validation MAE still remains ~ 0.033 - 0.035 when we slightly tuned the hyperparameters differently. This might be due to the fact that our cost function was changed, so a completely different combination of hyperparameters (therefore more careful tuning) is needed. **Bottom line is that this still leaves us with room for further improving model performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the feature importance plots from XGBoost:  \n",
    "![](images/XGBoost1.png)  \n",
    "<strong>Figure 8:</strong> <em> XGBoost: Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And LightGBM:\n",
    "\n",
    "![](images/LightGBM1.png)  \n",
    "<strong>Figure 9:</strong> <em> LightGBM: Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above feature importance plots we can see that although exact orders are different, both models \"adopt similar gaming strategy\" by assigning highest importance to roughly three categories of features: (1) Number of kills/total damages; (2) Distance walked/swam, etc.; and (3) Number of health items consumed (heals, boosts). We can, therefore, interpret this as \"the difference in these three aspects influence most in the final winning placement\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial features\n",
    "\n",
    "We started with a dataset with 29 columns (description of the features according to https://www.kaggle.com/c/pubg-finish-placement-prediction/data):\n",
    "\n",
    "-  **DBNOs** - Number of enemy players knocked.\n",
    "\n",
    "-  **assists** - Number of enemy players this player damaged that were killed by teammates.\n",
    "\n",
    "-  **boosts** - Number of boost items used.\n",
    "\n",
    "-  **damageDealt** - Total damage dealt. Note: Self inflicted damage is subtracted.\n",
    "\n",
    "-  **headshotKills** - Number of enemy players killed with headshots.\n",
    "\n",
    "-  **heals** - Number of healing items used.\n",
    "\n",
    "-  **Id** - Player’s Id\n",
    "\n",
    "-  **killPlace** - Ranking in the match of the number of enemy players killed.\n",
    "\n",
    "-  **killPoints** - Kills-based external ranking of the player. (Think of this as an Elo ranking where only kills matter.) If there is a value other than -1 in rankPoints, then any 0 in killPoints should be treated as a “None”.\n",
    "\n",
    "-  **killStreaks** - Max number of enemy players killed in a short amount of time.\n",
    "\n",
    "-  **kills** - Number of enemy players killed.\n",
    "\n",
    "-  **longestKill** - Longest distance between player and player killed at the time of death. This may be misleading, as downing a player and driving away may lead to a large longestKill stat.\n",
    "\n",
    "-  **matchDuration** - Duration of the match in seconds.\n",
    "\n",
    "-  **matchId** - ID to identify match. There are no matches that are in both the training and testing set.\n",
    "\n",
    "-  **matchType** - String identifying the game mode that the data comes from. The standard modes are “solo”, “duo”, “squad”, “solo-fpp”, “duo-fpp”, and “squad-fpp”; other modes are from events or custom matches.\n",
    "\n",
    "-  **rankPoints** - Elo-like ranking of the player. This ranking is inconsistent and is being deprecated in the API’s next version, so use with caution. Value of -1 takes place of “None”.\n",
    "\n",
    "-  **revives** - Number of times this player revived teammates.\n",
    "\n",
    "-  **rideDistance** - Total distance traveled in vehicles measured in meters.\n",
    "\n",
    "-  **roadKills** - Number of kills while in a vehicle.\n",
    "\n",
    "-  **swimDistance** - Total distance traveled by swimming measured in meters.\n",
    "\n",
    "-  **teamKills** - Number of times this player killed a teammate.\n",
    "\n",
    "-  **vehicleDestroys** - Number of vehicles destroyed.\n",
    "\n",
    "-  **walkDistance** - Total distance traveled on foot measured in meters.\n",
    "\n",
    "-  **weaponsAcquired** - Number of weapons picked up.\n",
    "\n",
    "-  **winPoints** - Win-based external ranking of player. (Think of this as an Elo ranking where only winning matters.) If there is a value other than -1 in rankPoints, then any 0 in winPoints should be treated as a “None”.\n",
    "\n",
    "-  **groupId** - ID to identify a group within a match. If the same group of players plays in different matches, they will have a different groupId each time.\n",
    "\n",
    "-  **numGroups** - Number of groups we have data for in the match.\n",
    "\n",
    "-  **maxPlace** - Worst placement we have data for in the match. This may not match with numGroups, as sometimes the data skips over placements.\n",
    "\n",
    "-  **winPlacePerc** - The target of prediction. This is a percentile winning placement, where 1 corresponds to 1st place, and 0 corresponds to the last place in the match. It is calculated off of maxPlace, not numGroups, so it is possible to have missing chunks in a match.\n",
    "\n",
    "Among these columns, **winPlacePerc** is our response variable i.e. the labels, while  **Id, matchId, groupId and matchType** are not informative features. Therefore, we have $29 - 5 = 24$ features. It is a fairly low number and we are looking forward to **increase the dimension** by introducing new features.\n",
    "\n",
    "## Additional features\n",
    "\n",
    "We first computed 11 new features based on our existing features as we thought these could be explaining our data better:\n",
    "\n",
    "-  **headshotrate = kills / headshotKills**\n",
    "\n",
    "-  **killStreakrate = killStreaks / kills**\n",
    "\n",
    "-  **healthitems = heals + boosts**\n",
    "    \n",
    "-  **totalDistance = rideDistance + walkDistance + swimDistance**\n",
    "\n",
    "-  **killPlace_over_maxPlace = killPlace / maxPlace**\n",
    "    \n",
    "-  **headshotKills_over_kills = headshotKills / kills**\n",
    "\n",
    "-  **distance_over_weapons = totalDistance / weaponsAcquired**\n",
    "\n",
    "-  **walkDistance_over_heals = walkDistance / heals**\n",
    "\n",
    "-  **walkDistance_over_kills = walkDistance / kills**\n",
    "\n",
    "-  **killsPerWalkDistance = kills / walkDistance**\n",
    "   \n",
    "-  **skill = headshotKills + roadKills**\n",
    "\n",
    "And in total now we have $24 + 11 = 35$ features.\n",
    "\n",
    "## Group dataset by individual player groups/teams and matches\n",
    "\n",
    "The original data is very noisy including 4,446,960 anonymized players' data. Since the majority of players participate in the game in groups/teams, it makes more sense to look at the groups as a whole instead of treating the teammates individually, since they may be cooperating with each other more and affecting each others' performance. Also, it does not make sense to compare different matches as the percentile winning placement and all other features were computed within each match. In consideration of the above two points, we decided to **group our dataset by each individual match id as well as group id, and recompute the features by summarising them into player teams (groups) and matches**. \n",
    "\n",
    "For each of the 35 features we had previously, we computed their **group-wise sum, mean, maximum and minimum as well as the respective ranks (4 summary statistics + 4 rank-based values, total 8 columns for each feature)**. We also computed the **match-wise mean for each of the features**. This results in 35 * 9 = 315 features. Finally, we computed the **size of individual matches and groups and make the two additional features**, resulting in a final matrix consisting of **1,621,392 grouped observations and 317 features**.\n",
    "\n",
    "## Standard Normalization\n",
    "\n",
    "For all the baseline models, deep neural networks, and ResNet,  we used the standard normalization to put all the features on the same scale. \n",
    "\n",
    "## PCA sphereing\n",
    "\n",
    "We also attempted to PCA sphere our data in order to standardize each one of our features. We compared the model performance before and after PCA sphereing. However, PCA sphereing resulted in higher MAE (mean absolute error).\n",
    "\n",
    "## Post Processing & small-sized training\n",
    "\n",
    "For all the baseline models, deep neural networks, and ResNet, we normalized the predictions to the range between 0 and 1 through MinMaxScaler [14]. As for the small-sized training, in order to optimize our models efficiently and ensure our implementation correct and can be optimized, we always trained our models on the small-sized dataset (splitting from the original training dataset, like including 10000 players' stats) at the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion  \n",
    "![](images/Performance1.png)  \n",
    "<strong>Figure 10:</strong> <em> Model Performance regarding MAE  \n",
    "\n",
    "From the previous results, we can conclude that the performance of different optimizers depends on the specific model. Practically we should choose the best combination of optimizers and costs. In terms of final model performance, advanced models generally perform better than baseline models, among which LightGBM performs the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference  \n",
    "[1] https://scikit-learn.org/stable/  \n",
    "[2] https://scikit-learn.org/stable/modules/tree.html#tree  \n",
    "[3] Schmidhuber, J. (2015). \"Deep Learning in Neural Networks: An Overview\". Neural Networks. 61: 85–117.  \n",
    "[4] Bengio, Yoshua (2009). \"Learning Deep Architectures for AI\". Foundations and Trends in Machine Learning. 2 (1): 1–127.  \n",
    "[5] https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks  \n",
    "[6] Gers, Felix A., Schmidhuber, Jürgen (2001). \"LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages\". IEEE Trans Neural Netw. 12 (6): 1333–1340.  \n",
    "[7] LeCun, Y., et al. (1998). \"Gradient-based learning applied to document recognition\". Proceedings of the IEEE. 86 (11): 2278–2324.  \n",
    "[8] Ioffe, Sergey, and Christian Szegedy. \"Batch normalization: Accelerating deep network training by reducing internal covariate shift.\" arXiv preprint arXiv:1502.03167 (2015).  \n",
    "[9] Maas, Andrew L., Awni Y. Hannun, and Andrew Y. Ng. \"Rectifier nonlinearities improve neural network acoustic models.\" Proc. icml. Vol. 30. No. 1. 2013.  \n",
    "[10] He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\" Proceedings of the IEEE international conference on computer vision. 2015.  \n",
    "[11] He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.  \n",
    "[12] Chen, Tianqi, and Carlos Guestrin. \"Xgboost: A scalable tree boosting system.\" Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. ACM, 2016.  \n",
    "[13] Ke, Guolin, et al. \"Lightgbm: A highly efficient gradient boosting decision tree.\" Advances in Neural Information Processing Systems. 2017.  \n",
    "[14] https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
